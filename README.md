
# CA-TCC: Few-Shot Learning for Exercise Recognition

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

**Self-Supervised Contrastive Learning for Exercise Type Classification with Limited Labeled Data**

This repository implements CA-TCC (Contrastive learning with Augmentation and Temporal Coherence) for learning robust video-level representations from wearable IMU sensor data.
After self-supervised pretraining, the model is fine-tuned with a small number of labeled samples for exercise type classification.

---

## ğŸ¯ Key Results

### Video-Level Accuracy (5 Seeds, Mean Â± Std)

| Method | 0-shot | 1-shot | 5-shot | 100% (Upper Bound) |
|--------|--------|--------|--------|--------------------|
| **Supervised Baseline** | â€” | 43.89% Â± 10.27 | 68.61% Â± 3.11 | **81.34% Â± 4.34** |
| **CA-TCC (Pretrain+FT)** | 12.20% Â± 6.27 | 47.61% Â± 11.82 | **71.43% Â± 3.36** | â€” |
| **Improvement** | â€” | +3.73% (+8.5%) | +2.83% (+4.1%) | â€” |

### Key Findings
1. **0-shot evaluation reveals poor transfer without fine-tuning:**
   - Only 12.20% accuracy with pretrained model (random: 10%)
   - Self-supervised pretraining alone is insufficient for this task
   - Fine-tuning with labeled data is essential
=======
1. **5-shot CA-TCC achieves best finetuning performance:**
   - 88.96% accuracy vs. 75.68% baseline
   - **+13.28 percentage points** absolute improvement
   - **+17.5% relative improvement**
   - Highly statistically significant (p<0.001)

2. **Modest improvements with few-shot learning:**
   - **5-shot**: CA-TCC achieves 71.43% vs. 68.61% baseline (+2.83pp, +4.1% relative)
   - **1-shot**: CA-TCC achieves 47.61% vs. 43.89% baseline (+3.73pp, +8.5% relative)

3. **Gap to upper bound:**
   - 5-shot CA-TCC: 71.43% vs. 100% supervised: 81.34%
   - Still ~10 percentage points below full supervision
   - Pretraining helps but doesn't close the gap completely

4. **Statistical significance:**
   - CATCC_5shot vs Supervised_5shot: p=0.0009 (highly significant ***)
   - CATCC_1shot vs Supervised_1shot: p=0.6468 (not significant)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Experimental Workflow](#experimental-workflow)
- [Dataset](#dataset)
- [Model Architecture](#model-architecture)
- [Augmentations](#augmentations)
- [Results](#results)
- [Citation](#citation)

---

## ğŸ” Overview

### Problem

Traditional supervised learning requires large amounts of labeled data. Can self-supervised pretraining enable learning from just a few labeled examples per class?

### Solution: Two-Stage Learning

**Stage 1: Self-Supervised Pretraining**
- Train on ALL unlabeled data (train + validation videos)
- Use temporal contrastive learning (CA-TCC)
- Learn general representations without labels

**Stage 2: Fine-Tuning (or Evaluation)**
- **0-shot**: Evaluate pretrained model directly (NO fine-tuning)
- **K-shot**: Fine-tune with K labeled videos per class
- **Supervised baseline**: Train from scratch with K videos per class (no pretraining)

### Why "Video-Level"?

- **1-shot** = 1 complete exercise video per class (10 videos total)
- **5-shot** = 5 videos per class from different subjects (50 videos)

Benefits:
- âœ… Balanced representation across all exercise types
- âœ… No data leakage (windows from same video stay together)
- âœ… Clear semantic meaning ("N labeled trials per exercise")

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8+
- CUDA-compatible GPU (recommended)
- 16GB+ RAM

### Install Dependencies

```bash
# Clone repository
git clone https://github.com/yourusername/CA-TCC-FewShot.git
cd CA-TCC-FewShot

# Install requirements
pip install -r requirements.txt
```

**Requirements:**
```
torch>=1.10.0
numpy>=1.20.0
pandas>=1.3.0
scikit-learn>=0.24.0
scipy>=1.7.0
```

---

## âš¡ Quick Start

### Step 1: Prepare Data for Multiple Seeds

```bash
# Generate 5 different random subject splits (seeds 0-4)
bash prepare_all_seeds.sh
```

This creates:
- `data/ExerciseIMU_seed0/` through `data/ExerciseIMU_seed4/`
- Each seed has different train/val/test subject splits
- Files: `pretrain.pt`, `train.pt`, `train_1shot.pt`, `train_5shot.pt`, `val.pt`, `test.pt`

### Step 2: Run All Experiments

```bash
# Run complete workflow for all 5 seeds
bash run_fewshot_experiments.sh
```

This runs for each seed:
1. Self-supervised pretraining (train+val, NO test)
2. 0-shot evaluation (pretrained model, NO fine-tuning)
3. 1-shot fine-tuning (pretrain â†’ 1-shot)
4. 5-shot fine-tuning (pretrain â†’ 5-shot)
5. Supervised 1-shot baseline (train from scratch)
6. Supervised 5-shot baseline (train from scratch)

**Note**: Full training set (100%) supervised baseline already exists in experiments.

### Step 3: Compare Results

```bash
# Aggregate results across all seeds
python compare_results_video.py
```

Results are saved to: `experiments_logs/FewShot_ExerciseIMU/comparison_results.txt`

---

## ğŸ“Š Experimental Workflow

### Complete Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Preparation (prepare_all_seeds.sh)                    â”‚
â”‚  - Generate 5 seeds with different subject splits           â”‚
â”‚  - Create pretrain.pt (train+val), train_Xshot.pt, test.pt â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For Each Seed (run_fewshot_experiments.sh):                â”‚
â”‚                                                              â”‚
â”‚  1. Self-Supervised Pretraining                             â”‚
â”‚     - Input: pretrain.pt (train+val videos, NO test)        â”‚
â”‚     - Method: Temporal contrastive learning (40 epochs)     â”‚
â”‚     - Output: Pretrained encoder weights                    â”‚
â”‚                                                              â”‚
â”‚  2. 0-Shot Evaluation                                        â”‚
â”‚     - Load pretrained weights                                â”‚
â”‚     - Evaluate on test set (NO training/fine-tuning)        â”‚
â”‚                                                              â”‚
â”‚  3. Few-Shot Fine-Tuning (1-shot, 5-shot)                   â”‚
â”‚     - Load pretrained weights                                â”‚
â”‚     - Fine-tune with K-shot labeled data (40 epochs)        â”‚
â”‚     - Evaluate on test set                                   â”‚
â”‚                                                              â”‚
â”‚  4. Supervised Baselines (1-shot, 5-shot)                   â”‚
â”‚     - Random initialization (NO pretraining)                 â”‚
â”‚     - Train from scratch with K-shot data (40 epochs)       â”‚
â”‚     - Evaluate on test set                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Results Aggregation (compare_results_video.py)             â”‚
â”‚  - Collect results from all seeds                           â”‚
â”‚  - Compute mean Â± std across seeds                          â”‚
â”‚  - Statistical significance testing (t-tests)               â”‚
â”‚  - Generate comparison tables                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Modes

| Mode | Description | Pretraining | Training Data | Purpose |
|------|-------------|-------------|---------------|---------|
| `self_supervised` | Pretrain encoder | â€” | pretrain.pt (train+val) | Stage 1 |
| `0shot` | Evaluate pretrained model | âœ… | None (just eval) | Baseline |
| `ft_1shot` | Fine-tune with 1-shot | âœ… | train_1shot.pt | Few-shot |
| `ft_5shot` | Fine-tune with 5-shot | âœ… | train_5shot.pt | Few-shot |
| `supervised_1shot` | Train from scratch | âŒ | train_1shot.pt | Baseline |
| `supervised_5shot` | Train from scratch | âŒ | train_5shot.pt | Baseline |
| `supervised` | Full supervision | âŒ | train.pt (full) | Upper bound |

---

## ğŸ“ Project Structure

```
CA-TCC/
â”œâ”€â”€ ğŸ“„ Main Scripts
â”‚   â”œâ”€â”€ main_video.py                      # Training script
â”‚   â”œâ”€â”€ prepare_exercise_data_video_v2.py  # Data preparation
â”‚   â”œâ”€â”€ prepare_all_seeds.sh               # Generate multiple seeds
â”‚   â”œâ”€â”€ run_fewshot_experiments.sh         # Run all experiments
â”‚   â””â”€â”€ compare_results_video.py           # Results analysis
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ Source Code
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ model.py                       # CNN encoder
â”‚   â”‚   â”œâ”€â”€ TC.py                          # Temporal contrastive module
â”‚   â”‚   â””â”€â”€ loss.py                        # Contrastive loss functions
â”‚   â”œâ”€â”€ dataloader/
â”‚   â”‚   â”œâ”€â”€ dataloader_video.py           # Video-level dataloader
â”‚   â”‚   â””â”€â”€ augmentations.py              # Time-series augmentations
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â””â”€â”€ trainer.py                     # Training loop
â”‚   â”œâ”€â”€ config_files/
â”‚   â”‚   â””â”€â”€ ExerciseIMU_Configs.py        # Hyperparameters
â”‚   â””â”€â”€ utils.py                           # Metrics, logging
â”‚
â””â”€â”€ ğŸ’¾ Data (Create Locally)
    â””â”€â”€ data/
        â”œâ”€â”€ ExerciseIMU_seed0/
        â”‚   â”œâ”€â”€ pretrain.pt                # Train+val (for pretraining)
        â”‚   â”œâ”€â”€ train.pt                   # Full training set
        â”‚   â”œâ”€â”€ train_1shot.pt             # 1 video per class
        â”‚   â”œâ”€â”€ train_5shot.pt             # 5 videos per class
        â”‚   â”œâ”€â”€ val.pt                     # Validation set
        â”‚   â””â”€â”€ test.pt                    # Test set
        â”œâ”€â”€ ExerciseIMU_seed1/
        â””â”€â”€ ... (seed2-4)
```

---

## ğŸ“Š Dataset

### Dataset Description

- **Exercises**: 10 resistance exercises (Bench Press, Deadlift, Overhead Press, etc.)
- **Subjects**: 13 participants
- **Sensors**: Bilateral wrist-worn IMU (left + right)
  - 12 channels total: 3-axis accelerometer + 3-axis gyroscope per wrist
- **Sampling rate**: 66 Hz
- **Window size**: 5 seconds (330 frames) with 2-second stride

### Subject-Level Data Splits (Per Seed)

| Split | Subjects | Videos | Windows | Purpose |
|-------|----------|--------|---------|---------|
| **Train** | 5 | ~210 | ~1,564 | Few-shot selection + full training |
| **Validation** | 4 | ~139 | ~1,187 | Hyperparameter tuning |
| **Test** | 4 | ~149 | ~1,131 | Final evaluation |

**Important**: Each seed has completely different subject assignments to train/val/test for robust evaluation.

### Few-Shot Sampling Strategy

**1-shot per class:**
- Select 1 random subject
- Take 1 random video from that subject for each class
- Total: 10 videos (1 per class)

**5-shot per class:**
- Select 5 random subjects (all different)
- Take 1 random video from each subject for each class
- Total: 50 videos (5 per class)

**Ensures**: Subject diversity and balanced class representation.

---

## ğŸ§  Model Architecture

### CA-TCC Framework

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: [Batch, 12 channels, 330 timesteps]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Encoder (3-layer 1D CNN)                                    â”‚
â”‚                                                              â”‚
â”‚  Conv Block 1: Conv1D(12â†’32, k=8) + BN + ReLU + MaxPool    â”‚
â”‚  Conv Block 2: Conv1D(32â†’64, k=8) + BN + ReLU + MaxPool    â”‚
â”‚  Conv Block 3: Conv1D(64â†’128, k=8) + BN + ReLU + MaxPool   â”‚
â”‚  Dropout: 0.35                                               â”‚
â”‚                                                              â”‚
â”‚  Output: [Batch, 128, 43] â†’ Flatten â†’ [Batch, 5504]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Temporal Contrastive Module (TC)                           â”‚
â”‚  - Transformer encoder (6 timesteps)                         â”‚
â”‚  - Enforces temporal coherence across augmented views       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Classification Head                                         â”‚
â”‚  - Linear(5504 â†’ 10 classes)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Configuration

| Parameter | Value |
|-----------|-------|
| Epochs | 40 |
| Batch size | 128 |
| Optimizer | Adam |
| Learning rate | 3e-4 |
| Î²1, Î²2 | 0.9, 0.99 |
| Weight decay | 3e-4 |
| Dropout | 0.35 |
| LR Scheduler | ReduceLROnPlateau (monitor val loss) |

---

## ğŸ”„ Augmentations

### Weak Augmentation
**Scaling** (applied to pretrain/SupCon modes)
- Randomly scale each channel by factor ~ Normal(Î¼=2.0, Ïƒ=1.1)
- Simulates variations in sensor sensitivity
- Less disruptive, preserves temporal structure

### Strong Augmentation
**Permutation + Jitter** (applied to pretrain/SupCon modes)
1. **Permutation**:
   - Split time series into 1-8 random segments
   - Randomly shuffle segment order
   - Simulates temporal variations

2. **Jitter**:
   - Add Gaussian noise ~ Normal(Î¼=0.0, Ïƒ=0.8)
   - Simulates sensor measurement noise

---

## ğŸ“ˆ Results

### Statistical Testing

All comparisons use two-tailed independent t-tests (n=5 seeds):

| Comparison | Window-Level | Video-Level | Significance |
|------------|--------------|-------------|--------------|
| **CATCC_5shot vs Supervised_5shot** | p=0.0004 | p=0.0009 | *** (Highly significant) |
| **CATCC_1shot vs Supervised_1shot** | p=0.6355 | p=0.6468 | ns (Not significant) |
| **0shot vs Supervised_1shot** | p=0.0008 | p=0.0008 | *** (Highly significant, worse) |

Significance levels: `***` p<0.001, `**` p<0.01, `*` p<0.05, `ns` = not significant

### Window vs. Video-Level Accuracy

| Method | Window-Level | Video-Level | Difference |
|--------|--------------|-------------|------------|
| **0-shot** | 13.27% Â± 6.78 | 12.20% Â± 6.27 | -1.07% |
| **Supervised 1-shot** | 43.19% Â± 9.35 | 43.89% Â± 10.27 | +0.70% |
| **CATCC 1-shot** | 47.07% Â± 12.67 | 47.61% Â± 11.82 | +0.54% |
| **Supervised 5-shot** | 67.86% Â± 3.43 | 68.61% Â± 3.11 | +0.75% |
| **CATCC 5-shot** | 71.83% Â± 2.96 | 71.43% Â± 3.36 | -0.40% |
| **Supervised 100%** | 80.37% Â± 3.48 | 81.34% Â± 4.34 | +0.97% |

**Observation**: Window and video-level accuracies are nearly identical, indicating stable and consistent predictions across windows within the same video.

### Analysis

**Why is 0-shot so poor (12.20%)?**
- Random baseline: 10% (10 classes)
- Self-supervised pretraining learns temporal features but NOT class-discriminative features
- The pretrained encoder requires fine-tuning to map features to specific exercise classes
- Without labeled data, the model cannot distinguish between exercise types

**Why are improvements modest?**
- Small labeled training sets (10 or 50 videos) limit fine-tuning effectiveness
- High inter-subject variability in exercise execution
- Some exercises are inherently similar (e.g., Overhead Press vs. Bench Press)
- Pretraining helps but doesn't fully overcome data scarcity

---

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@article{catcc2023,
  title={Self-Supervised Contrastive Representation Learning for Semi-Supervised Time-Series Classification},
  author={Eldele, Emadeldeen and Ragab, Mohamed and Chen, Zhenghua and Wu, Min and Kwoh, Chee-Keong and Li, Xiaoli and Guan, Cuntai},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={12},
  pages={15604--15618},
  year={2023},
  doi={10.1109/TPAMI.2023.3308189}
}
```

**Original CA-TCC paper:**
```bibtex
@inproceedings{tstcc2021,
  title={Time-Series Representation Learning via Temporal and Contextual Contrasting},
  author={Eldele, Emadeldeen and Ragab, Mohamed and Chen, Zhenghua and Wu, Min and Kwoh, Chee Keong and Li, Xiaoli and Guan, Cuntai},
  booktitle={IJCAI},
  pages={2352--2359},
  year={2021}
}
```

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Original CA-TCC implementation: [emadeldeen24/TS-TCC](https://github.com/emadeldeen24/TS-TCC)
- Few-shot learning extension and video-level evaluation: This work

---

**Last Updated:** November 14, 2025




